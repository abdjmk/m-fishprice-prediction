{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "plt.style.use('fivethirtyeight')\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data_set_process)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note！ that the input data name is data_set_process\n",
    "！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic model, not default parameters, optimized for data volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data_set_process)*0.8) #Partitioned data sets  0.8/0.2\n",
    "test_size = len(data_set_process) - train_size\n",
    "train_XGB, test_XGB = scaled_data[0:train_size,:],scaled_data[train_size:len(data_set_process),:]\n",
    "\n",
    "train_XGB_X, train_XGB_Y = train_XGB[:,:(len(data_set_process.columns)-1)],train_XGB[:,(len(data_set_process.columns)-1)]\n",
    "test_XGB_X, test_XGB_Y = test_XGB[:,:(len(data_set_process.columns)-1)],test_XGB[:,(len(data_set_process.columns)-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm parameters\n",
    "params = {\n",
    "    'booster':'gbtree',\n",
    "    'objective':'binary:logistic',  # 此处为回归预测，这里如果改成multi:softmax 则可以进行多分类\n",
    "    'gamma':0.1,\n",
    "    'max_depth':5,\n",
    "    'lambda':3,\n",
    "    'subsample':0.7,\n",
    "    'colsample_bytree':0.7,\n",
    "    'min_child_weight':3,\n",
    "    'slient':1,\n",
    "    'eta':0.1,\n",
    "    'seed':1000,\n",
    "    'nthread':8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate dataset format\n",
    "xgb_train = xgb.DMatrix(train_XGB_X,label = train_XGB_Y)\n",
    "xgb_test = xgb.DMatrix(test_XGB_X,label = test_XGB_Y)\n",
    "num_rounds = 300\n",
    "watchlist = [(xgb_test,'eval'),(xgb_train,'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost Model Training\n",
    "model_xgb = xgb.train(params,xgb_train,num_rounds,watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on the test set\n",
    "y_pred_xgb = model_xgb.predict(xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_xgb = mean_absolute_error(test_XGB_Y, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(test_XGB_Y, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse)\n",
    "r2_xgb = r2_score(test_XGB_Y, y_pred_xgb)\n",
    "\n",
    "print('MAE: {:.4f}'.format(mae_xgb))\n",
    "print('MSE: {:.4f}'.format(mse_xgb))\n",
    "print('RMSE: {:.4f}'.format(rmse_xgb))\n",
    "print('R^2: {:.4f}'.format(r2_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_xgb = np.mean(np.abs(y_pred_xgb-test_XGB_Y)/test_XGB_Y)*100\n",
    "print('The average error rate of XGBoost is：{}%'.format(mape_xgb))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_XGB_Y, color = 'red', label = 'Real for Test set')\n",
    "plt.plot(y_pred_xgb, color = 'blue', label = 'Predicted for Test set')\n",
    "plt.title('Prediction for Test set')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Predicted results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GridSearchCV in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm parameters\n",
    "params = {\n",
    "    'booster': ['gbtree'],\n",
    "    'objective': ['binary:logistic'],  # Here is the regression prediction, here if you change to multi:softmax, then you can do multiple classification\n",
    "    'gamma': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'lambda': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'eta': [0.05, 0.1, 0.2],\n",
    "    'seed': [1000],\n",
    "    'nthread': [8],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create XGBoost model\n",
    "xgb_model_CV = xgb.XGBRegressor()\n",
    "\n",
    "#GridSearchCV with 5-fold cross-validation\n",
    "grid = GridSearchCV(estimator=xgb_model_CV, param_grid=params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "#Fit the training data to the model\n",
    "grid.fit(train_XGB_X, train_XGB_Y)\n",
    "\n",
    "#Best parameters and score from GridSearchCV\n",
    "print(\"Best parameters found: \", grid.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid.best_score_)))\n",
    "\n",
    "#Use the best parameters to train the model\n",
    "best_params = grid.best_params_\n",
    "xgb_model_best = xgb.train(best_params, xgb_train, num_rounds, watchlist)\n",
    "\n",
    "#Make predictions on the test set\n",
    "y_pred_xgb_best = xgb_model_best.predict(xgb_test)\n",
    "mape_xgb_best = np.mean(np.abs(y_pred_xgb-test_XGB_Y)/test_XGB_Y)*100\n",
    "print('The average error rate of XGBoost with best parameters is: {}%'.format(mape_xgb_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_xgb_best = mean_absolute_error(test_XGB_Y, y_pred_xgb_best)\n",
    "mse_xgb_best = mean_squared_error(test_XGB_Y, y_pred_xgb_best)\n",
    "rmse_xgb_best = np.sqrt(mse)\n",
    "r2_xgb_best = r2_score(test_XGB_Y, y_pred_xgb_best)\n",
    "\n",
    "\n",
    "print('MAE: {:.4f}'.format(mae_xgb_best))\n",
    "print('MSE: {:.4f}'.format(mse_xgb_best))\n",
    "print('RMSE: {:.4f}'.format(rmse_xgb_best))\n",
    "print('R^2: {:.4f}'.format(r2_xgb_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_XGB_Y, color = 'red', label = 'Real for Test set')\n",
    "plt.plot(y_pred_xgb_best, color = 'blue', label = 'Predicted for Test set')\n",
    "plt.title('Prediction for Test set')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Predicted results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
